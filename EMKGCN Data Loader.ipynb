{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MKGCNDataset:\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        userID = np.array(self.dataset.iloc[idx]['userID'])\n",
    "        itemID = np.array(self.dataset.iloc[idx]['itemID'])\n",
    "        label = np.array(self.dataset.iloc[idx]['label'], dtype=np.float32)\n",
    "        return userID, itemID, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MKGCNDataLoader:\n",
    "    def __init__(self, args):\n",
    "        self.dataset_config = args.dataset_config\n",
    "\n",
    "        self.itemID2entityID = pd.read_csv(self.dataset_config[args.dataset]['itemID2entityID_path'],\n",
    "                                           sep=self.dataset_config[args.dataset]['itemID2entityID_sep'],\n",
    "                                           header=None, names=['itemID', 'entityID'], skiprows=1)\n",
    "        self.kg = pd.read_csv(self.dataset_config[args.dataset]['kg_path'],\n",
    "                              sep=self.dataset_config[args.dataset]['kg_sep'],\n",
    "                              header=None, names=['head', 'relation', 'tail'], skiprows=1)\n",
    "        self.userID2itemID4ratings = pd.read_csv(self.dataset_config[args.dataset]['userID2itemID4ratings_path'],\n",
    "                                                 sep=self.dataset_config[args.dataset]['userID2itemID4ratings_sep'],\n",
    "                                                 names=['userID', 'itemID', 'rating'], skiprows=1)\n",
    "\n",
    "        self.userID2itemID4ratings = self.userID2itemID4ratings[\n",
    "            self.userID2itemID4ratings['itemID'].isin(self.itemID2entityID['itemID'])]\n",
    "        self.userID2itemID4ratings.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        self.user_encoder = LabelEncoder()\n",
    "        self.entity_encoder = LabelEncoder()\n",
    "        self.relation_encoder = LabelEncoder()\n",
    "\n",
    "        self._build_num_info()\n",
    "        self._build_dataset(args)\n",
    "        self._build_negative_sampling(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_num_info(self):\n",
    "        print('Building numerical information...')\n",
    "        self.user_encoder.fit(self.userID2itemID4ratings['userID'])\n",
    "        self.entity_encoder.fit(pd.concat([self.itemID2entityID['entityID'], self.kg['head'], self.kg['tail']]))\n",
    "        self.relation_encoder.fit(self.kg['relation'])\n",
    "        self.kg['head'] = self.entity_encoder.transform(self.kg['head'])\n",
    "        self.kg['tail'] = self.entity_encoder.transform(self.kg['tail'])\n",
    "        self.kg['relation'] = self.relation_encoder.transform(self.kg['relation'])\n",
    "        self.num_user = len(self.user_encoder.classes_)\n",
    "        self.num_item = len(set(self.itemID2entityID['itemID']))\n",
    "        self.num_entity = len(self.entity_encoder.classes_)\n",
    "        self.num_relation = len(self.relation_encoder.classes_)\n",
    "        print('Numerical information built.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_dataset(self, args):\n",
    "        print('Building dataset...')\n",
    "        self.dataset = pd.DataFrame()\n",
    "        self.dataset['userID'] = self.user_encoder.transform(self.userID2itemID4ratings['userID'])\n",
    "        itemID2entityID_dict = dict(zip(self.itemID2entityID['itemID'], self.itemID2entityID['entityID']))\n",
    "        self.full_item_set = set(self.userID2itemID4ratings['itemID'].apply(lambda x: itemID2entityID_dict[x]))\n",
    "        self.userID2itemID4ratings['itemID'] = self.userID2itemID4ratings['itemID'].apply(\n",
    "            lambda x: itemID2entityID_dict[x])\n",
    "        self.dataset['itemID'] = self.entity_encoder.transform(self.userID2itemID4ratings['itemID'])\n",
    "        self.dataset['label'] = self.userID2itemID4ratings['rating'].apply(\n",
    "            lambda x: 0 if x < self.dataset_config[args.dataset]['threshold'] else 1)\n",
    "        self.dataset = self.dataset[self.dataset['label'] == 1]\n",
    "        print('Dataset built.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_negative_sampling(self, args):\n",
    "        print('Performing negative sampling...')\n",
    "        user_list = []\n",
    "        item_list = []\n",
    "        label_list = []\n",
    "        if args.sampling_strategy == 'proportional':\n",
    "            item_pop = self.dataset['itemID'].value_counts()\n",
    "            alpha = 1\n",
    "            item_prob = {item: 1 / (pop ** alpha) for item, pop in item_pop.items()}\n",
    "            item_prob = {k: v / sum(item_prob.values()) for k, v in item_prob.items()}\n",
    "        for user, group in self.dataset.groupby(['userID']):\n",
    "            item_set = set(group['itemID'])\n",
    "            if args.sampling_strategy == 'proportional':\n",
    "                negative_sampled = np.random.choice(list(item_prob.keys()), size=len(item_set), replace=False,\n",
    "                                                    p=list(item_prob.values()))\n",
    "            else:\n",
    "                negative_set = self.full_item_set - item_set\n",
    "                negative_sampled = random.sample(negative_set, len(item_set))\n",
    "            user_list.extend([user] * len(negative_sampled))\n",
    "            item_list.extend(negative_sampled)\n",
    "            label_list.extend([0] * len(negative_sampled))\n",
    "        negative = pd.DataFrame({'userID': user_list, 'itemID': item_list, 'label': label_list})\n",
    "        self.dataset = pd.concat([self.dataset, negative])\n",
    "        self.dataset = self.dataset.sample(frac=1, replace=False, random_state=999)\n",
    "        self.dataset.reset_index(inplace=True, drop=True)\n",
    "        print('Negative sampling done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_user_history_dict(self):\n",
    "        print('Loading user history dictionary...')\n",
    "        user_history_dict = dict()\n",
    "        for index in self.dataset.index:\n",
    "            user = self.dataset.loc[index].values[0]\n",
    "            item = self.dataset.loc[index].values[1]\n",
    "            if user not in user_history_dict:\n",
    "                user_history_dict[user] = []\n",
    "            user_history_dict[user].append(item)\n",
    "        print('User history dictionary loaded.')\n",
    "        return user_history_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_knowledge_graph(self):\n",
    "        print('Loading knowledge graph...')\n",
    "        kg = dict()\n",
    "        for i in range(len(self.kg)):\n",
    "            head = self.kg.iloc[i]['head']\n",
    "            relation = self.kg.iloc[i]['relation']\n",
    "            tail = self.kg.iloc[i]['tail']\n",
    "            if head in kg:\n",
    "                kg[head].append((relation, tail))\n",
    "            else:\n",
    "                kg[head] = [(relation, tail)]\n",
    "            if tail in kg:\n",
    "                kg[tail].append((relation, head))\n",
    "            else:\n",
    "                kg[tail] = [(relation, head)]\n",
    "        print('Knowledge graph loaded.')\n",
    "        return kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_info(self):\n",
    "        print('Loading dataset info...')\n",
    "        print('Dataset info loaded.')\n",
    "        return len(self.user_encoder.classes_), self.num_item, len(self.entity_encoder.classes_), len(\n",
    "            self.relation_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(self, args):\n",
    "        print('Loading dataset...')\n",
    "        dataset = self.dataset\n",
    "        train_dataset, test_dataset, train_label_dataset, test_label_dataset = train_test_split(dataset,\n",
    "                                                                                                dataset['label'],\n",
    "                                                                                                test_size=1 - args.ratio,\n",
    "                                                                                                shuffle=False,\n",
    "                                                                                                random_state=999)\n",
    "        train_mkgcn_dataset = MKGCNDataset(train_dataset)\n",
    "        test_mkgcn_dataset = MKGCNDataset(test_dataset)\n",
    "        train_loader = DataLoader(train_mkgcn_dataset, batch_size=args.batch_size)\n",
    "        test_loader = DataLoader(test_mkgcn_dataset, batch_size=args.batch_size)\n",
    "        print('Dataset loaded.')\n",
    "        return train_loader, test_loader, train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is loaded\n"
     ]
    }
   ],
   "source": [
    "def load_modals_dict(self, args):\n",
    "        print('Loading modal dictionary...')\n",
    "        modals = dict()\n",
    "        for i in range(len(args.modals)):\n",
    "            with open(self.dataset_config[args.dataset]['modals_path'].format(str(i)), 'r') as f:\n",
    "                for line in f:\n",
    "                    line = line.strip().split(self.dataset_config[args.dataset]['modals_sep'])\n",
    "                    item_id = line[0]\n",
    "                    features = list(map(float, line[1:]))\n",
    "                    if item_id not in modals:\n",
    "                        modals[item_id] = [[] for _ in range(len(args.modals))]\n",
    "                    modals[item_id][i] = features\n",
    "        print('Modal dictionary loaded.')\n",
    "        return modals\n",
    "\n",
    "print(\"Data is loaded\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
